# ğŸ“„ app.py
import streamlit as st
from dotenv import load_dotenv
from typing import Annotated, TypedDict
from langgraph.graph import StateGraph, END
from langchain_core.runnables import RunnableLambda
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI

# Load API key
load_dotenv()

# âœ… ìƒíƒœ ì •ì˜
class GraphState(TypedDict):
    question: Annotated[str, "question"]
    context: Annotated[str, "context"]
    answer: Annotated[str, "answer"]

# âœ… GPT ëª¨ë¸ ì„¤ì •
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# âœ… ë…¸ë“œ 1: ì§ˆë¬¸ + context ìƒì„±
def handle_question(state: GraphState) -> GraphState:
    # ì§ˆë¬¸ì€ ê³ ì •
    state["question"] = (
        "í…Œì´ë¸” ì»¬ëŸ¼ì˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬í˜• ì»¬ëŸ¼ì´ ë¬´ì—‡ì¸ì§€ ì•Œë ¤ì¤˜. "
        "ë‹µë³€ì€ ì»¬ëŸ¼ëª…ë§Œ ë‚˜ì—´í•´ì¤˜."
    )
    return state

# âœ… ë…¸ë“œ 2: GPT API í˜¸ì¶œ
def generate_answer(state: GraphState) -> GraphState:
    st.info("ğŸ’¬ GPTê°€ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")
    messages = [
        HumanMessage(content=f"ì§ˆë¬¸: {state['question']}\në¬¸ë§¥: {state['context']}")
    ]
    response = llm.invoke(messages)
    state["answer"] = response.content
    return state

# âœ… LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
workflow = StateGraph(GraphState)
workflow.add_node("handle_question", RunnableLambda(handle_question))
workflow.add_node("generate_answer", RunnableLambda(generate_answer))
workflow.set_entry_point("handle_question")
workflow.add_edge("handle_question", "generate_answer")
workflow.add_edge("generate_answer", END)
app = workflow.compile()

# âœ… Streamlit UI
st.title("ğŸ“Š í…Œì´ë¸” ë¶„ì„ ë„ìš°ë¯¸")
st.write("í…Œì´ë¸” ì»¬ëŸ¼ ì •ë³´(context)ë¥¼ ì…ë ¥í•˜ë©´, ì¹´í…Œê³ ë¦¬í˜• ì»¬ëŸ¼ì„ ë¶„ì„í•´ ë“œë¦½ë‹ˆë‹¤.")

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_context = st.text_area("ğŸ“¥ í…Œì´ë¸” ì •ë³´ (context)ë¥¼ ì…ë ¥í•˜ì„¸ìš”", height=200, placeholder="ì˜ˆ: í…Œì´ë¸”ëª… : cust_info, ì»¬ëŸ¼ëª… : [ê³ ê°ì•„ì´ë””,ì´ë¦„,ë‚˜ì´,...]")

if st.button("ë‹µë³€ ìƒì„±"):
    if not user_context.strip():
        st.warning("âš ï¸ í…Œì´ë¸” ì •ë³´(context)ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        # ì´ˆê¸° ìƒíƒœ
        input_state = {
            "question": "",       # ì§ˆë¬¸ì€ handle_questionì—ì„œ ê³ ì •
            "context": user_context,
            "answer": ""
        }

        # ê·¸ë˜í”„ ì‹¤í–‰
        result = app.invoke(input_state)

        # ê²°ê³¼ ì¶œë ¥
        st.success("âœ… ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:")
        st.markdown(f"**ğŸ§  GPT ë‹µë³€:**\n{result['answer']}")