import os
import tempfile
import streamlit as st
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader

load_dotenv()

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)

st.set_page_config(page_title="PDF QA ì±—ë´‡", page_icon="ğŸ“š", layout="wide")
st.title("ğŸ“š PDF ê¸°ë°˜ ëŒ€í™”í˜• ì±—ë´‡")
st.write("PDFë¥¼ ì—…ë¡œë“œí•˜ê³ , ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ëŒ€í™”í•˜ë“¯ ì§ˆë¬¸í•˜ì„¸ìš”.")

# ì„¸ì…˜ ìƒíƒœ
if "messages" not in st.session_state:
    st.session_state.messages = []
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        input_key="question",   # â˜… ì¤‘ìš”
        output_key="answer"     # â˜… ì¤‘ìš”
    )
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None

# PDF ì—…ë¡œë“œ
uploaded_file = st.sidebar.file_uploader("ğŸ“‚ PDF ì—…ë¡œë“œ", type=["pdf"])
if uploaded_file:
    st.sidebar.success(f"ì—…ë¡œë“œë¨: {uploaded_file.name}")

    # ì„ì‹œ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name

    # ë¡œë“œ/ë¶„í• 
    docs = PyPDFLoader(tmp_path).load()
    splits = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)

    # ì„ë² ë”©/ë²¡í„°ìŠ¤í† ì–´
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(splits, embeddings)

    # ëŒ€í™”í˜• RAG ì²´ì¸ (ë©”ëª¨ë¦¬ í¬í•¨)
    st.session_state.qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        memory=st.session_state.memory,          # â† ë©”ëª¨ë¦¬ ì£¼ì…
        return_source_documents=True
    )

# ê¸°ì¡´ ëŒ€í™” ë Œë”ë§
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        if st.session_state.qa_chain is None:
            answer = "ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš” ğŸ“‚"
        else:
            result = st.session_state.qa_chain({"question": prompt})  # â† 'question' í‚¤ ì‚¬ìš©
            answer = result["answer"]
            # ì°¸ê³ : result["source_documents"] ì—ì„œ ì¶œì²˜ ì—´ëŒ ê°€ëŠ¥
        st.markdown(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})

# ì´ˆê¸°í™”
if st.sidebar.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
    st.session_state.messages = []
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        input_key="question",
        output_key="answer"
    )
    st.session_state.qa_chain = None
    st.rerun()