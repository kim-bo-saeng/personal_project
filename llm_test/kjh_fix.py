import os
import tempfile
from dotenv import load_dotenv

# LangChain - OpenAI ëª¨ë¸
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain, RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, TextLoader

# ê¸°íƒ€
from langdetect import detect
import streamlit as st


# ==========================
# ğŸ”¹ í™˜ê²½ë³€ìˆ˜ ë° ëª¨ë¸ ì´ˆê¸°í™”
# ==========================
load_dotenv()
llm = ChatOpenAI(
    temperature=0.7,
    model="gpt-3.5-turbo"
)


# ==========================
# ğŸ“Œ ë¬¸ì„œ ë¡œë“œ ë° RAG ì²´ì¸ êµ¬ì„±
# ==========================
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

def build_chain_from_files(uploaded_files):
    documents = []
    for file in uploaded_files:
        # ì—…ë¡œë“œ íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        suffix = "." + file.name.split(".")[-1]
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
            tmp_file.write(file.read())
            tmp_path = tmp_file.name

        # íŒŒì¼ í™•ì¥ì í™•ì¸ í›„ ë¡œë“œ
        if file.name.endswith(".pdf"):
            loader = PyPDFLoader(tmp_path)
        elif file.name.endswith(".txt"):
            loader = TextLoader(tmp_path, encoding="utf-8")
        else:
            continue
        documents.extend(loader.load())

    if not documents:
        return None, 0

    splits = text_splitter.split_documents(documents)
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(splits, embeddings)

    chain_qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(),
        return_source_documents=True
    )
    return chain_qa, len(splits)


def query(chain_qa, question):
    result = chain_qa({"query": question})
    return result["result"]


# ==========================
# ğŸ“Œ ì™¸êµ­ì–´ ì…ë ¥ ì²˜ë¦¬
# ==========================
def translate_to_korean(text: str) -> str:
    return text  # TODO: ì‹¤ì œ ë²ˆì—­ API ì—°ê²°

def translate_from_korean(text: str, target_lang: str) -> str:
    return text  # TODO: ì‹¤ì œ ë²ˆì—­ API ì—°ê²°

def foreign_chatbot(message):
    detected_language = detect(message)
    if detected_language != "ko":
        translated_message = translate_to_korean(message)
        response = st.session_state.conversation.predict(input=translated_message)
        return translate_from_korean(response, detected_language)
    return st.session_state.conversation.predict(input=message)


# ==========================
# ğŸ“Œ Streamlit UI
# ==========================
st.set_page_config(
    page_title="ì±—ë´‡",
    page_icon="ğŸ’¬",
    layout="wide"
)

if "messages" not in st.session_state:
    st.session_state.messages = []

if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory()
    st.session_state.conversation = ConversationChain(
        memory=st.session_state.memory,
        llm=llm
    )

if "chain_qa" not in st.session_state:
    st.session_state.chain_qa = None

st.title("ğŸ“š ë¬¸ì„œ ê¸°ë°˜ ì±—ë´‡")
st.write("PDF ë˜ëŠ” TXT íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³ , ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”!")

# ğŸ”¹ íŒŒì¼ ì—…ë¡œë“œ UI
uploaded_files = st.sidebar.file_uploader(
    "íŒŒì¼ ì—…ë¡œë“œ (PDF/TXT ì§€ì›)", 
    type=["pdf", "txt"], 
    accept_multiple_files=True
)

if uploaded_files:
    st.sidebar.success(f"{len(uploaded_files)}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ âœ…")
    chain_qa, n_chunks = build_chain_from_files(uploaded_files)
    if chain_qa:
        st.session_state.chain_qa = chain_qa
        st.sidebar.write(f"ë¬¸ì„œê°€ {n_chunks} ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        st.sidebar.error("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

# ğŸ”¹ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ğŸ”¹ ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        if st.session_state.chain_qa:
            answer = query(st.session_state.chain_qa, prompt)
        else:
            answer = st.session_state.conversation.predict(input=prompt)
        st.markdown(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})

# ğŸ”¹ ì´ˆê¸°í™” ë²„íŠ¼
if st.sidebar.button("ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
    st.session_state.messages = []
    st.session_state.memory = ConversationBufferMemory()
    st.session_state.conversation = ConversationChain(
        memory=st.session_state.memory,
        llm=llm
    )
    st.session_state.chain_qa = None
    st.rerun()