{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터를 불러와서 학습할 수 있는 형태로 변환해주는 단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 활용할 최초의 데이터를 불러오는 작업\n",
    "with open('abalone.csv') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader, None)\n",
    "    rows = []\n",
    "    for row in csvreader:\n",
    "        rows.append(row)\n",
    "\n",
    "# 인풋과 아웃풋의 차원을 설정해 주고 data를 텐서화 시켜주는 작업\n",
    "input_cnt , output_cnt = 10,1\n",
    "data = np.zeros([len(rows),input_cnt+output_cnt])\n",
    "\n",
    "# 불러온 데이터를 0으로 채워진 텐서구조에 넣어주기\n",
    "for n, row in enumerate(rows):\n",
    "    if row[0] == \"I\" : data[n,0] = 1\n",
    "    if row[0] == \"M\" : data[n,1] = 1\n",
    "    if row[0] == \"F\" : data[n,2] = 1\n",
    "    data[n,3:] = row[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4177, 11)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 weight,bias를 설정해줌 : 정규분포를 갖는 난수값으로 초기 설정을 해줌)\n",
    "# 정규분포 난수값의 평균과 표준편차를 설정하여 가중치의 파라미터를 초기화 할때 사용한다\n",
    "RND_MEAN = 0\n",
    "RND_STD = 0.0030\n",
    "\n",
    "# 학습률을 설정(하이퍼파라미터)\n",
    "LEARNING_LATE = 0.001\n",
    "\n",
    "weight = np.random.normal(RND_MEAN,RND_STD,[input_cnt , output_cnt])\n",
    "bias = np.zeros([output_cnt])\n",
    "\n",
    "# 에포크 횟수(전체 학습횟수)와 미니배치 크기(한번에 학습하는 크기)를 설정하고 리포트 보고 주기를 설정\n",
    "epoch_count = 10\n",
    "mb_size = 10\n",
    "report = 1\n",
    "\n",
    "# 데이터를 섞고 랜덤으로 추출하여 학습하기 위한 데이터를 준비\n",
    "shuffle_map = np.arange(data.shape[0])                   # 데이터하나하나에 순번을 붙여주는 작업\n",
    "np.random.shuffle(shuffle_map)                           # 순번을 섞는 작업\n",
    "step_count = int(data.shape[0] * 0.8) // mb_size         # 데이터를 한번에 학습할 크기로 전체크기를 나눠 전체를 학습하기 위해 몇회를 해야되는지의 값\n",
    "test_begin_idx = step_count * mb_size                    # 테스트 데이터가 시작될 시작점의 번호(해당 인덱스를 기준으로 학습데이터와 테스트 데이터를 나누면됨)\n",
    "\n",
    "# 데이터를 학습및 테스트\n",
    "for epoch in range(epoch_count):\n",
    "    losses, accs = [], []\n",
    "\n",
    "    for n in range(step_count):\n",
    "        if n == 0:\n",
    "            np.random.shuffle(shuffle_map[:test_begin_idx])                   # 에포크가 한번 끝나고 다시 학습을 할때마다 데이터를 섞어준다, 데이터는 8:2로 나눠져있으며 인덱스값으로 구분\n",
    "        train_x = data[shuffle_map[mb_size*n:mb_size*(n+1)]][:, :-output_cnt] # 데이터에서 가장 마지막값을 제외한 나머지 값이 학습 데이터이다 => x\n",
    "        train_y = data[shuffle_map[mb_size*n:mb_size*(n+1)]][:, -output_cnt:] # 데이터에서 가장 마지막값은 정답인 정답데이터 이다 => y\n",
    "        \n",
    "        \n",
    "        # forward path 연산\n",
    "        output = np.matmul(train_x,weight) + bias\n",
    "        aux_nn = train_x\n",
    "        \n",
    "        diff = output - train_y\n",
    "        loss = np.mean(np.square(diff))\n",
    "        aux_pp = diff\n",
    "\n",
    "        # step마다 loss와acc를 계산\n",
    "        accuracy = 1 - np.mean(np.abs((output - y)/y))\n",
    "        losses.append(loss)\n",
    "        accs.append(accuracy)\n",
    "\n",
    "        # backporpagation 연산(weight와 bias 업데이트)\n",
    "        G_loss = 1.0\n",
    "        \n",
    "        g_loss_suqare = np.ones(diff.shape) / np.prod(shape)\n",
    "        \n",
    "        G_output = \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
