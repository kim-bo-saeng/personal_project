{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터를 불러와서 학습할 수 있는 형태로 변환해주는 단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 활용할 최초의 데이터를 불러오는 작업\n",
    "with open('abalone.csv') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader, None)\n",
    "    rows = []\n",
    "    for row in csvreader:\n",
    "        rows.append(row)\n",
    "\n",
    "# 인풋과 아웃풋의 차원을 설정해 주고 data를 텐서화 시켜주는 작업\n",
    "input_cnt , output_cnt = 10,1\n",
    "data = np.zeros([len(rows),input_cnt+output_cnt])\n",
    "\n",
    "# 불러온 데이터를 0으로 채워진 텐서구조에 넣어주기\n",
    "for n, row in enumerate(rows):\n",
    "    if row[0] == \"I\" : data[n,0] = 1\n",
    "    if row[0] == \"M\" : data[n,1] = 1\n",
    "    if row[0] == \"F\" : data[n,2] = 1\n",
    "    data[n,3:] = row[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=33.563, accuracy=0.556/0.816\n",
      "Epoch 2: loss=8.078, accuracy=0.818/0.817\n",
      "Epoch 3: loss=7.363, accuracy=0.813/0.809\n",
      "Epoch 4: loss=7.241, accuracy=0.809/0.809\n",
      "Epoch 5: loss=7.153, accuracy=0.809/0.809\n",
      "Epoch 6: loss=7.078, accuracy=0.811/0.806\n",
      "Epoch 7: loss=7.017, accuracy=0.809/0.807\n",
      "Epoch 8: loss=6.961, accuracy=0.810/0.808\n",
      "Epoch 9: loss=6.913, accuracy=0.810/0.806\n",
      "Epoch 10: loss=6.871, accuracy=0.809/0.807\n",
      "Epoch 11: loss=6.833, accuracy=0.810/0.806\n",
      "Epoch 12: loss=6.798, accuracy=0.810/0.806\n",
      "Epoch 13: loss=6.768, accuracy=0.809/0.807\n",
      "Epoch 14: loss=6.739, accuracy=0.810/0.807\n",
      "Epoch 15: loss=6.713, accuracy=0.810/0.807\n",
      "Epoch 16: loss=6.689, accuracy=0.810/0.808\n",
      "Epoch 17: loss=6.668, accuracy=0.811/0.806\n",
      "Epoch 18: loss=6.647, accuracy=0.811/0.805\n",
      "Epoch 19: loss=6.630, accuracy=0.810/0.806\n",
      "Epoch 20: loss=6.612, accuracy=0.810/0.806\n",
      "Epoch 21: loss=6.595, accuracy=0.810/0.807\n",
      "Epoch 22: loss=6.581, accuracy=0.811/0.806\n",
      "Epoch 23: loss=6.567, accuracy=0.811/0.805\n",
      "Epoch 24: loss=6.554, accuracy=0.810/0.806\n",
      "Epoch 25: loss=6.541, accuracy=0.810/0.807\n",
      "Epoch 26: loss=6.526, accuracy=0.812/0.804\n",
      "Epoch 27: loss=6.515, accuracy=0.810/0.807\n",
      "Epoch 28: loss=6.504, accuracy=0.811/0.807\n",
      "Epoch 29: loss=6.493, accuracy=0.812/0.806\n",
      "Epoch 30: loss=6.482, accuracy=0.811/0.807\n",
      "Epoch 31: loss=6.472, accuracy=0.810/0.808\n",
      "Epoch 32: loss=6.462, accuracy=0.812/0.806\n",
      "Epoch 33: loss=6.452, accuracy=0.811/0.807\n",
      "Epoch 34: loss=6.443, accuracy=0.812/0.806\n",
      "Epoch 35: loss=6.434, accuracy=0.811/0.807\n",
      "Epoch 36: loss=6.423, accuracy=0.811/0.807\n",
      "Epoch 37: loss=6.414, accuracy=0.811/0.808\n",
      "Epoch 38: loss=6.406, accuracy=0.812/0.807\n",
      "Epoch 39: loss=6.398, accuracy=0.812/0.807\n",
      "Epoch 40: loss=6.389, accuracy=0.812/0.807\n",
      "Epoch 41: loss=6.379, accuracy=0.811/0.808\n",
      "Epoch 42: loss=6.371, accuracy=0.813/0.806\n",
      "Epoch 43: loss=6.363, accuracy=0.811/0.809\n",
      "Epoch 44: loss=6.354, accuracy=0.813/0.806\n",
      "Epoch 45: loss=6.344, accuracy=0.813/0.805\n",
      "Epoch 46: loss=6.340, accuracy=0.811/0.808\n",
      "Epoch 47: loss=6.329, accuracy=0.813/0.805\n",
      "Epoch 48: loss=6.324, accuracy=0.811/0.807\n",
      "Epoch 49: loss=6.315, accuracy=0.812/0.808\n",
      "Epoch 50: loss=6.306, accuracy=0.813/0.807\n",
      "Epoch 51: loss=6.299, accuracy=0.813/0.808\n",
      "Epoch 52: loss=6.291, accuracy=0.812/0.809\n",
      "Epoch 53: loss=6.284, accuracy=0.813/0.808\n",
      "Epoch 54: loss=6.275, accuracy=0.813/0.809\n",
      "Epoch 55: loss=6.269, accuracy=0.813/0.809\n",
      "Epoch 56: loss=6.259, accuracy=0.814/0.807\n",
      "Epoch 57: loss=6.254, accuracy=0.813/0.808\n",
      "Epoch 58: loss=6.245, accuracy=0.814/0.807\n",
      "Epoch 59: loss=6.238, accuracy=0.812/0.810\n",
      "Epoch 60: loss=6.230, accuracy=0.814/0.809\n",
      "Epoch 61: loss=6.224, accuracy=0.813/0.810\n",
      "Epoch 62: loss=6.216, accuracy=0.814/0.810\n",
      "Epoch 63: loss=6.209, accuracy=0.814/0.810\n",
      "Epoch 64: loss=6.201, accuracy=0.814/0.809\n",
      "Epoch 65: loss=6.195, accuracy=0.814/0.810\n",
      "Epoch 66: loss=6.187, accuracy=0.814/0.811\n",
      "Epoch 67: loss=6.181, accuracy=0.815/0.811\n",
      "Epoch 68: loss=6.171, accuracy=0.816/0.808\n",
      "Epoch 69: loss=6.167, accuracy=0.814/0.810\n",
      "Epoch 70: loss=6.158, accuracy=0.815/0.811\n",
      "Epoch 71: loss=6.151, accuracy=0.816/0.809\n",
      "Epoch 72: loss=6.146, accuracy=0.815/0.809\n",
      "Epoch 73: loss=6.139, accuracy=0.815/0.810\n",
      "Epoch 74: loss=6.131, accuracy=0.814/0.812\n",
      "Epoch 75: loss=6.125, accuracy=0.816/0.811\n",
      "Epoch 76: loss=6.116, accuracy=0.815/0.813\n",
      "Epoch 77: loss=6.111, accuracy=0.816/0.813\n",
      "Epoch 78: loss=6.104, accuracy=0.816/0.812\n",
      "Epoch 79: loss=6.097, accuracy=0.815/0.813\n",
      "Epoch 80: loss=6.091, accuracy=0.816/0.812\n",
      "Epoch 81: loss=6.085, accuracy=0.816/0.812\n",
      "Epoch 82: loss=6.076, accuracy=0.816/0.812\n",
      "Epoch 83: loss=6.071, accuracy=0.816/0.812\n",
      "Epoch 84: loss=6.065, accuracy=0.816/0.813\n",
      "Epoch 85: loss=6.057, accuracy=0.817/0.812\n",
      "Epoch 86: loss=6.051, accuracy=0.817/0.811\n",
      "Epoch 87: loss=6.046, accuracy=0.816/0.813\n",
      "Epoch 88: loss=6.039, accuracy=0.817/0.813\n",
      "Epoch 89: loss=6.032, accuracy=0.816/0.813\n",
      "Epoch 90: loss=6.026, accuracy=0.817/0.813\n",
      "Epoch 91: loss=6.020, accuracy=0.817/0.813\n",
      "Epoch 92: loss=6.013, accuracy=0.816/0.814\n",
      "Epoch 93: loss=6.006, accuracy=0.817/0.815\n",
      "Epoch 94: loss=6.001, accuracy=0.817/0.814\n",
      "Epoch 95: loss=5.995, accuracy=0.818/0.814\n",
      "Epoch 96: loss=5.987, accuracy=0.817/0.816\n",
      "Epoch 97: loss=5.983, accuracy=0.818/0.815\n",
      "Epoch 98: loss=5.976, accuracy=0.817/0.816\n",
      "Epoch 99: loss=5.971, accuracy=0.818/0.815\n",
      "Epoch 100: loss=5.963, accuracy=0.818/0.815\n",
      "\n",
      "Final Test : final accuracy = 0.815\n"
     ]
    }
   ],
   "source": [
    "# 초기 weight,bias를 설정해줌 : 정규분포를 갖는 난수값으로 초기 설정을 해줌)\n",
    "# 정규분포 난수값의 평균과 표준편차를 설정하여 가중치의 파라미터를 초기화 할때 사용한다\n",
    "RND_MEAN = 0\n",
    "RND_STD = 0.0030\n",
    "\n",
    "# 학습률을 설정(하이퍼파라미터)\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "weight = np.random.normal(RND_MEAN,RND_STD,[input_cnt , output_cnt])\n",
    "bias = np.zeros([output_cnt])\n",
    "\n",
    "# 에포크 횟수(전체 학습횟수)와 미니배치 크기(한번에 학습하는 크기)를 설정하고 리포트 보고 주기를 설정\n",
    "epoch_count = 100\n",
    "mb_size = 10\n",
    "report = 1\n",
    "\n",
    "# 데이터를 섞고 랜덤으로 추출하여 학습하기 위한 데이터를 준비\n",
    "shuffle_map = np.arange(data.shape[0])                   # 데이터하나하나에 순번을 붙여주는 작업\n",
    "np.random.shuffle(shuffle_map)                           # 순번을 섞는 작업\n",
    "step_count = int(data.shape[0] * 0.8) // mb_size         # 데이터를 한번에 학습할 크기로 전체크기를 나눠 전체를 학습하기 위해 몇회를 해야되는지의 값\n",
    "test_begin_idx = step_count * mb_size                    # 테스트 데이터가 시작될 시작점의 번호(해당 인덱스를 기준으로 학습데이터와 테스트 데이터를 나누면됨)\n",
    "\n",
    "# 데이터를 학습및 테스트\n",
    "for epoch in range(epoch_count):\n",
    "    losses, accs = [], []\n",
    "\n",
    "    for n in range(step_count):\n",
    "        if n == 0:\n",
    "            np.random.shuffle(shuffle_map[:test_begin_idx])                   # 에포크가 한번 끝나고 다시 학습을 할때마다 데이터를 섞어준다, 데이터는 8:2로 나눠져있으며 인덱스값으로 구분\n",
    "        train_x = data[shuffle_map[mb_size*n:mb_size*(n+1)]][:, :-output_cnt] # 데이터에서 가장 마지막값을 제외한 나머지 값이 학습 데이터이다 => x\n",
    "        train_y = data[shuffle_map[mb_size*n:mb_size*(n+1)]][:, -output_cnt:] # 데이터에서 가장 마지막값은 정답인 정답데이터 이다 => y\n",
    "        \n",
    "        \n",
    "        # forward path 연산\n",
    "        output_train_x = np.matmul(train_x,weight) + bias\n",
    "        \n",
    "        diff = output_train_x - train_y\n",
    "        loss = np.mean(np.square(diff))\n",
    "\n",
    "        # step마다 loss와acc를 계산\n",
    "        accuracy = 1 - np.mean(np.abs((output_train_x - train_y)/train_y))\n",
    "        losses.append(loss)\n",
    "        accs.append(accuracy)\n",
    "\n",
    "        # backporpagation 연산(weight와 bias 업데이트)\n",
    "        G_loss = 1.0\n",
    "        \n",
    "        g_loss_suqare = np.ones(diff.shape) / np.prod(diff.shape)\n",
    "        g_square_diff = 2*diff\n",
    "        g_diff_outout = 1\n",
    "\n",
    "        G_square = g_loss_suqare * G_loss\n",
    "        G_diff = g_square_diff * G_square\n",
    "        G_output = g_diff_outout * G_diff\n",
    "\n",
    "        g_outout_w = train_x.transpose()\n",
    "\n",
    "        G_w = np.matmul(g_outout_w,G_output)\n",
    "        G_b = np.sum(G_output, axis=0)\n",
    "\n",
    "        weight -= LEARNING_RATE * G_w\n",
    "        bias -= LEARNING_RATE * G_b\n",
    "\n",
    "    if report > 0 and (epoch+1) % report == 0:\n",
    "        test_data = data[shuffle_map[test_begin_idx:]]\n",
    "        test_x = test_data[:, :-output_cnt]\n",
    "        test_y = test_data[:, -output_cnt:]\n",
    "        output_test_x = np.matmul(test_x,weight) + bias\n",
    "        acc= 1 - np.mean(np.abs((output_test_x - test_y)/test_y))\n",
    "        print('Epoch {}: loss={:5.3f}, accuracy={:5.3f}/{:5.3f}'. \\\n",
    "              format(epoch+1,np.mean(losses),np.mean(accs),acc))\n",
    "\n",
    "output_test_x = np.matmul(test_x,weight) + bias\n",
    "final_acc= 1 - np.mean(np.abs((output_test_x - test_y)/test_y))\n",
    "print('\\nFinal Test : final accuracy = {:5.3f}'.format(final_acc))      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
